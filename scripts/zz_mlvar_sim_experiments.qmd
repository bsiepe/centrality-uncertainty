---
title: "mlVAR Simulation"
format: html
---

# Background
In this document, we try to adapt the `mlVAR::mlVARsim` function to our purposes
```{r}
library(mvtnorm)
library(here)
library(mlVAR)
library(tidyverse)
source(here("scripts", "00_functions.R"))
```


# Trying mlVARsim_mod 
Copied from: https://github.com/SachaEpskamp/mlVAR/blob/master/R/mlVARmodel.R

```{r}
# Graph simulator:
simGraph <- function(
  Nvar,
  sparsity = 0.5,
  parRange = c(0.5,1),
  constant = 1.1,
  propPositive = 0.5
){
  ## Approach from 
  # Yin, J., & Li, H. (2011). A sparse conditional gaussian graphical model for analysis of genetical genomics data. The annals of applied statistics, 5(4), 2630.
  
  # Empty matrix:
  trueKappa <- matrix(0,Nvar,Nvar)
  
  # Total edges:
  totEdges <- sum(upper.tri(trueKappa))
  
  # Included edges:
  nEdges <- round((1-sparsity)*totEdges)
  
  # Sample the edges:
  inclEdges <- sample(seq_len(totEdges),nEdges)
  
  # Make edges:
  trueKappa[upper.tri(trueKappa)][inclEdges] <- 1
  
  # Make edges negative and add weights:
  trueKappa[upper.tri(trueKappa)] <- trueKappa[upper.tri(trueKappa)] * sample(c(-1,1),sum(upper.tri(trueKappa)),TRUE,prob=c(propPositive,1-propPositive)) * 
    runif(sum(upper.tri(trueKappa)), min(parRange ),max(parRange ))
  
  # Symmetrize:
  trueKappa[lower.tri(trueKappa)] <- t(trueKappa)[lower.tri(trueKappa)]  
  
  # Make pos def:
  diag(trueKappa) <- constant * rowSums(abs(trueKappa))
  diag(trueKappa) <- ifelse(diag(trueKappa)==0,1,diag(trueKappa))
  trueKappa <- trueKappa/diag(trueKappa)[row(trueKappa)]
  trueKappa <- (trueKappa + t(trueKappa)) / 2
  
  return(as.matrix(qgraph::wi2net(trueKappa)))
}



# 1. fixed effects (means) for every parameter and a variance-covariance matrix.
# 2. Generate Beta's 
# 3. Shrink all beta's until all eigenvalues are in unit circle.
# If I simulate with fixed effects variance of 1, then the effects are standardized?

# Now also implemented taking temporal fixed effects and means from a fitted mlVAR model
mlVARsim_mod <- function(
  # Simulation setup:
  nPerson = 10, # # of persons.
  nNode = 5, # # of nodes 
  nTime = 100, # Or vector with time points per person. 
  lag = 1,
  empirical_model = NULL, # use fitted mlVAR model fixed effects coefficients
  thetaVar = rep(1,nNode),
  DF_theta = nNode*2,
  mu_SD = c(1,1),
  init_beta_SD = c(0.1,1),
  fixedMuSD = 1,
  fixedBetaSD = 1,
  shrink_fixed = 0.9,
  shrink_deviation = 0.9,
  pcor_sparsity = 0.5,
  beta_sparsity = 0.5
){
  # browser()
  contemporaneous <- "wishart"
  GGMsparsity = pcor_sparsity
  
  if (length(nTime)==1){
    nTime <- rep(nTime,nPerson)
  }
  # Number of temporal effects:
  nTemporal <- nNode^2 * lag
  
  # 1. Generate structures
  # Generate Omega:

  # Simulate mu means:
  Omega_mu <- cov2cor(solve(diag(nNode)-simGraph(nNode)))
  # Simulate temporal:
  Omega_Beta <- cov2cor(solve(diag(nTemporal)-simGraph(nTemporal)))
  

  Omega <- rbind(
    cbind(Omega_mu, matrix(0,nNode,nTemporal)),
    cbind(matrix(0,nTemporal,nNode), Omega_Beta)
  )

  
  # Generate SD and scale:
  # BS: I think the nNode here may be wrong? it should be nNode + nTemporal
  # SD <- runif(nNode + nTemporal, c(rep(mu_SD[1],nNode),rep(init_beta_SD[1],nNode)), c(rep(mu_SD[2],nNode),rep(init_beta_SD[2],nNode)))
  SD <- runif(nNode + nTemporal, c(rep(mu_SD[1],nNode),rep(init_beta_SD[1],nTemporal)), c(rep(mu_SD[2],nNode),rep(init_beta_SD[2],nTemporal)))
  

  Omega <- diag(SD) %*%Omega %*% diag(SD)
  


  # Generate fixed contemporaneous:
  if (contemporaneous=="wishart"){
    Theta_fixed <- cov2cor(solve(diag(nNode)-simGraph(nNode)))
    Theta_fixed <- diag(sqrt(thetaVar)) %*% Theta_fixed %*% diag(sqrt(thetaVar))
    
    # 2. Generate residual covariance matrices:
    Theta <- rWishart(nPerson, DF_theta, Theta_fixed/DF_theta)
  } else {

    if (contemporaneous == "randomGGM"){
      Theta <- lapply(1:nPerson,function(x){
        net <- simGraph(nNode,GGMsparsity)
        cov2cor(solve(diag(nNode) - net))
      })
      Theta <- do.call(abind,c(Theta,along=3))
     
      Theta_fixed <- apply(Theta,1:2,mean)
    } else {
      net <- simGraph(nNode,GGMsparsity)
      Theta_fixed <- cov2cor(solve(diag(nNode) - net))
      Theta <- lapply(1:nPerson,function(x)Theta_fixed)
      Theta <- do.call(abind,c(Theta,along=3))
    }
  }
 
  # if no empirical model is provided
  if(is.null(empirical_model)){
  # Generate fixed means:
  mu_fixed <- rnorm(nNode,0,fixedMuSD)
  # Generate fixed betas:
  beta_fixed <- rnorm(nTemporal,0, fixedBetaSD)
  # set weakest beta_sparsity% to zero:
  beta_fixed[order(abs(beta_fixed))[1:round(nTemporal * beta_sparsity)]] <- 0
  
  # Include auto-regressions:
  mat <- matrix(0,nNode,nNode*lag)
  diag(mat) <- 1
  beta_fixed[c(mat)==1] <- runif(sum(c(mat)==1),0,1)
  }
  # If empirical model is provided for fixed effects
  if(!is.null(empirical_model)){
    mu_fixed <- empirical_model$results$mu$mean
    beta_fixed <- empirical_model$results$Beta$mean
  }
  
  
  # 3. Generate random parameter sets:
  if (lag > 0){
    try <- 1 
    maxtry <- 5000
    repeat{
      Pars <- rmvnorm(nPerson, c(mu_fixed,beta_fixed), sigma = Omega)
      Mus <- Pars[,1:nNode]
      
      Betas <- array(c(t(Pars[,-(1:nNode)])), c(nNode,nNode*lag,nPerson))
      
      # 4. Construct the matrices:
      if (lag>1){
        under <- cbind(diag(nNode*(lag-1)),matrix(0,nNode*(lag-1),nNode))
        
        ev <- sapply(seq_len(nPerson),function(i){
          mat <- rbind(Betas[,,i],under)
          eigen(mat)$values
        })
        
      } else {
        
        ev <- sapply(seq_len(nPerson),function(i){
          eigen(Betas[,,i])$values
        })
        
      }
      # 5. Store results:
      allEV <- c(ev)
      
      # 6. Break if all Re(ev)^2 + Im(ev)^2 < 1
      if (all(Re(ev)^2 + Im(ev)^2 < 1)){
        
        # simulate VAR for every person:
        DataList <- lapply(1:nPerson,function(p){
          
          pars <- lapply(seq_len(lag),function(l)array(c(Betas[,,p]),c(nNode,nNode,lag))[,,l])
          # If lag > 0 simulate VAR:
          if (lag > 0){
            res <- mlVAR::simulateVAR(pars, 
                                      means = Mus[p,], 
                                      lags = seq_len(lag), 
                                      Nt = nTime[p],
                                      init = Mus[p,],
                                      burnin = 100,
                                      residuals = Theta[,,p]) 
          } else {
            res <- rmvnorm(nTime[p],Mus[p,],Theta[,,p])
          }
          colnames(res) <- paste0("V",1:nNode)
          res$ID <- p
          res
        })
        
        # Rbind data:
        Data <- do.call(rbind,DataList)
        
        # 10. If any absolute > 100, go to 6a
        if (!any(abs(Data[,1:nNode]) > 100)){
          break
        }
      } 
      
      # Else shrink:
      beta_fixed <- beta_fixed * shrink_fixed
      D <- diag(sqrt(diag(Omega)))
      D[-(1:nNode),-(1:nNode)] <- shrink_deviation * D[-(1:nNode),-(1:nNode)] 
      Omega <- D %*% cov2cor(Omega) %*% D
    
      try <- try + 1
      if (try > maxtry){
        stop("Maximum number of tries reached.")
      }
    }
  }
    else {   # if lag = 0
    Pars <- rmvnorm(nPerson, mu_fixed, sigma = Omega)
    Mus <- Pars[,1:nNode]
    Betas <- array(dim = c(0,0,nPerson))
    
    # simulate VAR for every person:
    DataList <- lapply(1:nPerson,function(p){
      res <- as.data.frame(rmvnorm(nTime[p],Mus[p,],Theta[,,p]))
      colnames(res) <- paste0("V",1:nNode)
      res$ID <- p
      res
    })
    
    # Rbind data:
    Data <- do.call(rbind,DataList)
    
  }
  
  
  # Create the list:
  model <- list(
    mu = mlVAR:::modelArray(mean = mu_fixed, 
                            SD = mu_SD, 
                            subject = lapply(1:nrow(Mus), function(i)Mus[i,])),
    Beta = mlVAR:::modelArray(mean = array(beta_fixed,c(nNode,nNode,lag)), 
                              SD = array(sqrt(diag(Omega[-(1:nNode),-(1:nNode)])),c(nNode,nNode,lag)), 
                              subject = lapply(1:nPerson, 
                                               function(p)
                                                 array(Betas[,,p],c(nNode,nNode,lag)))),
    Omega_mu = mlVAR:::modelCov(
      cov = mlVAR:::modelArray(mean = Omega[1:nNode,1:nNode])
    ),
    Theta = mlVAR:::modelCov(
      cov = mlVAR:::modelArray(mean = Theta_fixed, 
                               subject = lapply(1:nPerson,function(p)Theta[,,p]))
    ),
    Omega = mlVAR:::modelCov(
      cov = mlVAR:::modelArray(mean = Omega)
    )
    
  )
  
  
  # Data generated! Now return in sensible list:
  Results <- list(
    Data = Data,
    vars = paste0("V",1:nNode),
    idvar = "ID",
    lag=lag,
    try = try,
model=model
  )
  
  class(Results) <- "mlVARsim"
  
  return(Results)
}


```


## Test mlVARsim_mod

```{r}
set.seed(123)
sim <- mlVARsim_mod(nPerson = 100, 
                    nNode = 9, 
                    nTime = 200, 
                    empirical_model = res3,
                    init_beta_SD = c(.1,4),
                    mu_SD = c(.1, .3),
                    shrink_deviation = 0.999,
                    fixedMuSD = 0.3,
                    fixedBetaSD = 1, 
                    pcor_sparsity = 0,
                    beta_sparsity = 0
                    # DF_theta = 1*6
                    )

sim_old <- mlVAR::mlVARsim(nPerson = 100, 
                    nNode = 6, 
                    nTime = 100, 
                    init_beta_SD = c(2,2)
                    )

```

Investigate heterogeneity of individual networks. Simulating with more heterogeneity does not really work. It seems like for the betas, only every second column is affected.

```{r}
ml_beta <- sim$model$Beta$subject
ml_pcor <- sim$model$Theta$pcor$subject

# across all matrices in the list, compute the element-wise sd
sd_beta <- sim$model$Beta$SD
sd_pcor <- sim$model$Theta$pcor$SD

# these are the empirical values
apply(simplify2array(ml_beta), c(1,2), sd)
apply(simplify2array(ml_pcor), c(1,2), sd)


# These are the simulation values
sd_beta
sd_pcor
```


# Trying simMLgvar

```{r}
library(graphicalVAR)
set.seed(2024)
sim_mlgvar <- simMLgvar(
                       nTime = 150, 
                       nVar = 6, 
                       nPerson = 200, 
                       propPositive = 0.5, 
                       kappaRange = c(0.25, 0.5),
                       betaRange = c(0.25, 0.5), 
                       betweenRange = c(0.25, 0.5),
                       rewireWithin = 0, 
                       betweenVar = 1, 
                       withinVar = 0.25,
                       temporalOffset = 2)

```
This does not look nice - cannot modify as much as I would like to. 







# Writing own function
Try to create a function that uses a fitted fixed-effects structure to simulate data with a specific random effects variance, but uses a graphicalVAR structure for each individual.
Taken from here: https://github.com/bsiepe/var-compare/blob/main/true-networks.Rmd

```{r}
# Graph to simulate from
graph_nonsparse <- readRDS(here("data/graph_nonsparse.RDS"))
graph_sparse <- readRDS(here("data/graph_sparse.RDS"))
```

This function should:
1. Take the fixed effects structure for beta and kappa
2. Create random effects for beta and for kappa 
3. Simulate data for each individual using the graphicalVAR 


```{r}
sim_gvar <- function(graph,
                     beta_sd,
                     kappa_sd,
                     n_person,
                     n_time,
                     n_node,
                     max_try = 1000){
  # browser()
  
  # Create a list to store the data for each person
  data <- vector("list", n_person)
  
  
  counter <- 0
    repeat {
      counter <- counter + 1
      if(counter > max_try) stop("Exceeded maximum number of attempts to generate stable beta matrix.")
      
      # Generate beta matrix using graph$beta as mean
      beta <- array(rnorm(n_node*n_node*n_person, mean = graph$beta, sd = beta_sd), c(n_node, n_node, n_person))
      
      # Check if beta matrix is stable
      ev_b <- sapply(seq_len(n_person),function(i){
          eigen(beta[,,i])$values
        })
      if(all(Re(ev_b)^2 + Im(ev_b)^2 < 1)) break
      
    }
    
    counter <- 0
    repeat {
      counter <- counter + 1
      if(counter > max_try) stop("Exceeded maximum number of attempts to generate semi-positive definite kappa matrix.")
      
      # Generate kappa matrix using graph$kappa as mean
      kappa <- array(rnorm(n_node*n_node*n_person, mean = graph$kappa, sd = kappa_sd), c(n_node, n_node, n_person))
      
      # Make kappa matrix symmetric
      # but keep as array
      force_symmetric <- function(mat) {
              as.matrix(Matrix::forceSymmetric(mat))
          }
      
      kappa <- array(apply(kappa, MARGIN = 3, FUN = force_symmetric), dim = dim(kappa)) 
      
      # Check if kappa matrix is semi-positive definite
      ev_k <- sapply(seq_len(n_person),function(i){
          eigen(kappa[,,i])$values
        })
      
      if(all(ev_k >= 0)) break
    }
    
    # Simulate data for each person
    for(i in seq_len(n_person)){
    data[[i]] <- tryCatch({graphicalVAR::graphicalVARsim(nTime = n_time,
                                               beta = beta[,,i],
                                               kappa = kappa[,,i]
                                               )}, error = function(e) NA)
    }
  
  
  # Return the list of simulated data
  # Also return the parameters used in the simulation
  ret <- list()
  ret$data <- data
  ret$beta <- beta
  ret$kappa <- kappa
  return(ret)

}
```

Try the function
```{r}
sim_data <- sim_gvar(graph = graph_nonsparse,
                     beta_sd = 0.1,
                     kappa_sd = 0.1,
                     n_person = 100,
                     n_time = 150,
                     n_node = 6,
                     max_try = 10000)
```

This still does not work with a larger amount of beta sd



## With for-loop approach

Idea: In the function above (`sim_gvar`), all beta and kappa matrices are generated at once.Instead, we could generate the beta and kappa matrices for each individual separately. This would allow us to check for stability and semi-positive definiteness for each individual separately. 



Try the function
```{r}
sim_data_loop <- sim_gvar_loop(
                     graph = graph_sparse,
                     beta_sd = 0.75,
                     kappa_sd = 0.3,
                     n_person = 100,
                     n_time = 150,
                     n_node = 6,
                     max_try = 10000,
                     sparse_sim = TRUE)

```

Try if it can be fitted
```{r}
data_loop <- sim_data_loop$data %>% 
  # convert each array in list to dataframe
  map(~as.data.frame(.x)) %>%
  bind_rows(.id = "id") 
  
var_labs <- c("V1", "V2", "V3", "V4", "V5", "V6")


loop_test_fit <- mlVAR(data_loop,
              vars=var_labs,
              idvar="id",
              lags = 1,
              temporal = "correlated",
              contemporaneous = "correlated",
              nCores = 13)
```


Check if it actually increases the random effects sd as intended

```{r}
# non-sparse Graph to simulate from
graph_nonsparse <- readRDS(here::here("data/graph_nonsparse.RDS"))

# sparse DGP
graph_sparse <- readRDS(here::here("data/graph_sparse.RDS"))

dgp <- "sparse"
heterogeneity <- "low"
heterogeneity <- "high"

dgp_graph <- ifelse(dgp == "sparse", 
                      "graph_sparse", 
                      "graph_nonsparse")
beta_sd <- ifelse(heterogeneity == "low",
                    0.05,
                    0.1)
  
n_id <- 50 
n_tp <- 75
n_var <- 6

# scale kappa random effects w.r.t diagonal elements
mean_diag_kappa <- mean(diag(graph_sparse$kappa)) 
kappa_sd_low <- 0.05 * mean_diag_kappa
kappa_sd_high <- 0.1 * mean_diag_kappa
  
kappa_sd <- ifelse(heterogeneity == "low",
                     kappa_sd_low,
                     kappa_sd_high)
  
ml_sim <- sim_gvar_loop(
                     graph = graph_sparse,
                     beta_sd = beta_sd,
                     kappa_sd = kappa_sd,
                     n_person = n_id,
                     n_time = n_tp,
                     n_node = n_var,
                     max_try = 10000,
                     listify = TRUE,
                     sparse_sim = TRUE)

# Check if the beta and kappa sd are as intended
ml_sim$beta %>% 
  apply(c(1,2), sd) %>% 
  mean()

ml_sim$pcor %>% 
  apply(c(1,2), sd) %>% 
  # remove diagonal 
  .[upper.tri(.)] %>% 
  mean()




```




## Try GIMME model fit 
Try it out to see how best to extract results
```{r}
gimme_fit <- gimme::gimme(ml_sim$data,
             ar = TRUE,
             subgroup = TRUE,
             plot = TRUE,
             hybrid = FALSE,
             VAR = TRUE,
             groupcutoff = .75,
             subcutoff = .75)

gimme_fit$path_se_est |> 
  # filter contemporaneous relationships
  filter(op == "~~") |>
  filter(pval < 0.05) |>
  mutate(file = str_remove(file, "subj")) %>% 
  filter(file == 1) %>% 
  select(lhs, rhs, beta.std) %>% 
  spread(lhs, beta.std) %>% 
  column_to_rownames(var = "rhs") %>%
  as.matrix()









centrality_gimme <- function(fit,
                             var_only = FALSE){  # only covariances in contemp?
  
  #--- Prepare 
  n_var <- fit$n_vars_total
  temp_ind <- 1:(n_var/2)
  cont_ind <- ((n_var/2)+1):n_var
  
  
  #--- Density
  dens_temp <- lapply(fit$path_est_mats, function(x){
    if(!is.double(x)){
      NA
    }
    else{
      sum(abs(x[, temp_ind]))/(n_var^2)
    }
  })
  
  dens_cont <- lapply(fit$path_est_mats, function(x){
    if(!is.double(x)){
      NA
    }
    else{
      x <- x[, cont_ind]
      diag(x) <- 0
      x[lower.tri(x)] <- 0L
      sum(abs(x))/ (n_var * (n_var-1)/2)
    }
  })

  #--- Centrality
  outstrength <- lapply(fit$path_est_mats, function(x){
    if(!is.double(x)){
      NA
    }
    else{
      colSums(abs(x[, temp_ind]))/n_var
    }
  })
  instrength <- lapply(fit$path_est_mats, function(x){
    if(!is.double(x)){
      NA
    }
    else{
      rowSums(abs(x[, temp_ind]))/n_var
    }
  })
  strength <- lapply(fit$path_est_mats, function(x){
    if(!is.double(x)){
      NA
    }
    else{
      # x <- x[, cont_ind]
      # diag(x) <- 0
      # x[lower.tri(x)] <- 0L
      colSums(abs(x))/n_var
    }
  })
  
  # return list
  return(list(outstrength = outstrength,
              instrength = instrength,
              strength = strength,
              dens_temp = dens_temp,
              dens_cont = dens_cont))
  
  
}

```



# Check implied centralities/densities 

Here, we investigate the implied centralities and densities when simulating with our data-generation function. 
```{r}
# non-sparse Graph to simulate from
graph_nonsparse <- readRDS(here::here("data/graph_nonsparse.RDS"))
graph_sparse <- readRDS(here::here("data/graph_sparse.RDS"))
graph_nonsparse_koval <- readRDS(here::here("data/graph_nonsparse_koval.RDS"))
graph_sparse_koval <- readRDS(here::here("data/graph_sparse_koval.RDS"))
dgp_graph <- graph_nonsparse_koval



beta_sd_low <- 0.05
beta_sd_high <- 0.1
mean_diag_kappa <- mean(diag(dgp_graph$kappa)) 
kappa_sd_low <- 0.05 * mean_diag_kappa
kappa_sd_high <- 0.1 * mean_diag_kappa
n_id <- 200
n_tp <- 100
n_var <- 6

# simulate models
ml_sim_low <- sim_gvar_loop(
                     graph = dgp_graph,
                     beta_sd = beta_sd_low,
                     kappa_sd = kappa_sd_low,
                     n_person = n_id,
                     n_time = n_tp,
                     n_node = n_var,
                     max_try = 10000,
                     listify = TRUE,
                     sparse_sim = TRUE)
ml_sim_high <- sim_gvar_loop(
                     graph = dgp_graph,
                     beta_sd = beta_sd_high,
                     kappa_sd = kappa_sd_high,
                     n_person = n_id,
                     n_time = n_tp,
                     n_node = n_var,
                     max_try = 10000,
                     listify = TRUE,
                     sparse_sim = TRUE)

# Obtain true centralities
true_cent_low <- centrality_mlvar_sim(ml_sim_low,
                                    sim_fn = "sim_gvar_loop")

true_cent_high <- centrality_mlvar_sim(ml_sim_high,
                                    sim_fn = "sim_gvar_loop")

```

## Centralities
Create plot for centralities: 
```{r}
# Oustrength
do.call(rbind, true_cent_low$outstrength) %>% 
  as.data.frame() |> 
  mutate(rand_sd = "low") |> 
  bind_rows(do.call(rbind, true_cent_high$outstrength) %>% 
              as.data.frame() |> 
              mutate(rand_sd = "high")) |>
  pivot_longer(!rand_sd, names_to = "Node", values_to = "Outstrength") |>
  ggplot(aes(x = Node, y = Outstrength)) +
  geom_point()+
  ggdist::stat_halfeye()+
  theme_centrality()+
  facet_wrap(~rand_sd)
  
# contemp. strength
do.call(rbind, true_cent_low$strength) %>% 
  as.data.frame() |> 
  mutate(rand_sd = "low") |> 
  bind_rows(do.call(rbind, true_cent_high$strength) %>% 
              as.data.frame() |> 
              mutate(rand_sd = "high")) |>
  pivot_longer(!rand_sd, names_to = "Node", values_to = "Strength") |>
  ggplot(aes(x = Node, y = Strength)) +
  geom_point()+
  ggdist::stat_halfeye()+
  theme_centrality()+
  facet_wrap(~rand_sd)

  
```

Additionally, we can calculate the difference between the most central and second most central node for each centrality measure
```{r}
# Outstrength
do.call(rbind, true_cent_low$outstrength) %>% 
  as.data.frame() |> 
  mutate(rand_sd = "low") |> 
  bind_rows(do.call(rbind, true_cent_high$outstrength) %>% 
              as.data.frame() |> 
              mutate(rand_sd = "high")) |>
  # add id indicator
  mutate(id = row_number()) |>
  pivot_longer(!c(id, rand_sd), names_to = "Node", values_to = "Outstrength") |>
  group_by(id, rand_sd) |>
  summarise(diff_out = max(Outstrength) - sort(Outstrength, decreasing = TRUE)[2]) |> 
  ggplot(aes(x = rand_sd, y = diff_out)) +
  geom_point() +
  ggdist::stat_halfeye()+
  theme_centrality() +
  labs(title = "Difference between most central and second most central edge",
       subtitle = "For outstrength")
```





## Densities

Create plot for densities
```{r}
df_dens <- data.frame(dens_temp = do.call(rbind, true_cent$dens_temp),
                      dens_cont = do.call(rbind, true_cent$dens_cont)) %>% 
  pivot_longer(everything(), names_to = "type", values_to = "density") |>
  mutate(type = ifelse(type == "dens_temp", "Temporal", "Contemporaneous"))

df_dens |>
  ggplot(aes(x = type, y = density)) +
  geom_point()+
  ggdist::stat_halfeye()+
  theme_centrality()

```
























