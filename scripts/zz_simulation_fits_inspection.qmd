---
title: "simulation_fits_inspection"
format: html
editor: source
---

# Prep

In this file, we investigate different simulation fits

```{r}
library(tidyverse)
library(lm.beta)
```

```{r}
# data from sim run on 2024-04-09
sim_res_4 <- readRDS("~/centrality-uncertainty/scripts/SimDesign-results_pc04798_2/results-row-4.rds")
```

# Analyze bmlVAR

Compare contemporaneous fits, which seemed off:

```{r}
ut <- function(x){
  x[upper.tri(x)]
}

sim_res_4$results[[1]]$bmlvar$fit_bmlvar$pcor[[2]]
ut(sim_res_4$results[[1]]$pcor[[2]])


```


Look at temporal fit:

```{r}
# Look at absolute differences across all individuals
Reduce("+", Map("abs", Map("-", sim_res_4$results[[1]]$bmlvar$fit_bmlvar$beta, sim_res_4$results[[1]]$beta))) / length(sim_res_4$results[[1]]$bmlvar$dens_temp)

# Only look at bias, not absolute
Reduce("+", Map("-", sim_res_4$results[[1]]$bmlvar$fit_bmlvar$beta, sim_res_4$results[[1]]$beta)) / length(sim_res_4$results[[1]]$bmlvar$dens_temp)


```



## Regression outcomes

Unstandardized regression coefs:
```{r}
sim_res_4$results[[1]]$bmlvar$reg_bmlvar$regression_slope$median

  
```

Standardized regression coefs: 

```{r}
get_filtered_summary <- function(result, iteration) {
  # Extract and filter the summary data
  result_df <- result$bmlvar$summary_bmlvar |> 
    as.data.frame() |> 
    rownames_to_column(var = "param") |> 
    filter(param %in% c("reg_slope_density_z[3]", 
                        "reg_slope_density_z[6]",
                        "reg_slope_density_z[9]",
                        "reg_slope_density[3]",
                        "reg_slope_density[6]",
                        "reg_slope_density[9]"))
  
  result_df$iteration <- iteration
  
  return(result_df)
}

summary_list <- lapply(seq_along(sim_res_4$results), function(i) {
  get_filtered_summary(sim_res_4$results[[i]], i)
})

final_summary_df <- bind_rows(summary_list)

final_summary_df

final_summary_df |> 
  filter(!grepl("_z", param)) |> 
  separate_wider_delim(param, delim = "[", names = c("param", "param_num")) |> 
  mutate(param_num = gsub("]", "", param_num)) |> 
  mutate(param_num = as.factor(param_num)) |> 
  rename(posterior_mean = mean,
         posterior_sd = sd) |> 
  group_by(param_num) |> 
  reframe(across(c(posterior_mean, posterior_sd, "2.5%", "97.5%"),
                   mean))

```




Plot them all (first need to account for the weird data structure where the results are a vector of length 9)

```{r}
slopes <- lapply(sim_res_4$results, function(x){
  tmp <- x$bmlvar$reg_bmlvar$regression_slope$median
  df_slopes <- data.frame(
    # true coefficients
    zero = tmp[c(1,4,7)],
    two = tmp[c(2,5,8)],
    four = tmp[c(3,6,9)]
  )
  df_slopes
})


slopes |> 
  bind_rows() |> 
  rename(
    "0.0" = zero,
    "0.2" = two,
    "0.4" = four
  ) |> 
  pivot_longer(cols = everything(), 
               names_to = "reg_coef") |> 
  mutate(reg_coef = as.factor(reg_coef)) |> 
  ggplot(aes(x = reg_coef, y = value)) + 
  geom_point()+
  labs(x = "True Regression Slope",
       y = "Estimated Regression Slope",
       title = "BmlVAR Estimates of Regresion Slope",
       caption = "n = 200") +
  theme_centrality()+
  theme(text = element_text(size = 21))

```


## Use two-step with BmlVAR centrality estimates

Try using centrality point estimates obtained with BmlVAR to estimate a normal `lm()`.

```{r}
get_lm_beta_coefficient <- function(result) {
  out_est <- sapply(result$bmlvar$outstrength, function(x) x[1])
  
  covar_true <- result$covariate_out_strength[, 4]
  
  df_out <- data.frame(
    outstrength = out_est, 
    covariate = covar_true
  )
  
  test <- lm.beta(lm(covariate ~ outstrength, data = df_out))
  return(test$coefficients[2])
}

coefficients_list <- lapply(sim_res_4$results, get_lm_beta_coefficient)
coefficients_vector <- unlist(coefficients_list)

coefficients_vector

```



# Analyze other models

## graphicalVAR

### Regression outcomes

```{r}
l_coefs <- list()
l_coefs$zero <- vector()
l_coefs$two <- vector()
l_coefs$four <- vector()

for(i in 1:10){
  l_coefs$zero[i] <- lm.beta(r1$results[[i]]$gvar$reg_outstrength[[1]])$standardized.coefficients[2]
  l_coefs$two[i] <- lm.beta(r1$results[[i]]$gvar$reg_outstrength[[2]])$standardized.coefficients[2]
  l_coefs$four[i] <- lm.beta(r1$results[[i]]$gvar$reg_outstrength[[3]])$standardized.coefficients[2]
}

l_coefs |> 
  dplyr::bind_rows(.id = "true_coef") |> 
    pivot_longer(cols = everything(), 
               names_to = "reg_coef") |> 
  mutate(reg_coef = as.factor(reg_coef)) |> 
  ggplot(aes(x = reg_coef, y = value)) + 
  geom_point()

```

## mlVAR

### Network Estimation

Calculate average difference

```{r}
# Temporal Network
Reduce("+", Map("abs", Map("-", sim_res_4$results[[1]]$mlvar$fit_mlvar$beta,sim_res_4$results[[1]]$beta))) / 200

# Contemporaneous Network
Reduce("+", Map("abs", Map("-", sim_res_4$results[[1]]$mlvar$fit_mlvar$pcor,sim_res_4$results[[1]]$pcor))) / 200
```

Create a long PDF with a heatmap of the differences for each individual, maybe we can find some patterns on why it didn't work:

```{r}
# Create a function to generate heatmaps and save them to a PDF
generate_heatmaps <- function(true_matrices, estimated_matrices, output_file, n_ind) {
  
  # Check that both lists are of the same length
  if (length(true_matrices) != length(estimated_matrices)) {
    stop("The lists of matrices must have the same length.")
  }
  
  # Open a PDF device
  pdf(file = output_file, width = 8, height = 10)
  
  # Loop over each pair of matrices
  for (i in 1:n_ind) {
    
    # Compute the difference matrix
    diff_matrix <- true_matrices[[i]] - estimated_matrices[[i]]
    
    # Convert the difference matrix to a data frame for ggplot
    diff_df <- reshape2::melt(diff_matrix)
    
    # Generate the heatmap
    p <- ggplot(diff_df, aes(x = Var1, y = Var2, fill = value)) +
      geom_tile() +
      scale_fill_gradient2(low = "darkblue", mid = "white", high = "darkred", midpoint = 0) +
      # plot the numerical value
      geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
      labs(title = paste("Difference Matrix for Individual", i),
           x = "",
           y = "",
           fill = "Difference") +
      theme_minimal()
    
    # Print the plot to the PDF
    print(p)
  }
  
  # Close the PDF device
  dev.off()
}

# Example usage
# Assuming true_matrices and estimated_matrices are defined
# true_matrices <- list(matrix(rnorm(25), nrow = 5), matrix(rnorm(25), nrow = 5))
# estimated_matrices <- list(matrix(rnorm(25), nrow = 5), matrix(rnorm(25), nrow = 5))

# Call the function
generate_heatmaps(sim_res_4$results[[1]]$beta, sim_res_4$results[[1]]$mlvar$fit_mlvar$beta, "heatmaps.pdf", n_ind = 200)

```



### Centrality

Somehow, the rank correlation for mlVAR is quite bad. Plot some examples:

```{r}
df_denstemp <- data.frame(
  mlvar_outstrength = sapply(sim_res_4$results[[3]]$mlvar$outstrength, function(x) x[[1]]),
  mlvar_instrength = sapply(sim_res_4$results[[3]]$mlvar$instrength, function(x) x[[1]]),
  true_outstrength =  sapply(sim_res_4$results[[3]]$true_cent$outstrength, function(x) x[[1]])
)

df_denstemp |> 
  ggplot(aes(x = true_outstrength, y = mlvar_instrength)) + 
  geom_point() + 
  geom_smooth()
```

Contemporaneous:

```{r}
df_contemp_mlvar <- data.frame(
  mlvar_strength = sapply(sim_res_4$results[[3]]$mlvar$strength, function(x) x[[1]]),
  true_strength =  sapply(sim_res_4$results[[3]]$true_cent$strength, function(x) x[[1]])
)

df_contemp_mlvar|> 
  ggplot(aes(x = true_strength, y = mlvar_strength)) + 
  geom_point() + 
  geom_smooth()
```



### Regression outcomes

```{r}
l_coefs <- list()
l_coefs$zero <- vector()
l_coefs$two <- vector()
l_coefs$four <- vector()

for(i in 1:10){
  l_coefs$zero[i] <- lm.beta(sim_res_4$results[[i]]$mlvar$reg_outstrength[[1]])$standardized.coefficients[2]
  l_coefs$two[i] <- lm.beta(sim_res_4$results[[i]]$mlvar$reg_outstrength[[2]])$standardized.coefficients[2]
  l_coefs$four[i] <- lm.beta(sim_res_4$results[[i]]$mlvar$reg_outstrength[[3]])$standardized.coefficients[2]
}

l_coefs |> 
  dplyr::bind_rows(.id = "true_coef") |> 
    pivot_longer(cols = everything(), 
               names_to = "reg_coef") |> 
  mutate(reg_coef = as.factor(reg_coef)) |> 
  ggplot(aes(x = reg_coef, y = value)) + 
  geom_point()

```

## GIMME

### Regression Outcomes

```{r}
l_coefs <- list()
l_coefs$zero <- vector()
l_coefs$two <- vector()
l_coefs$four <- vector()

for(i in 1:10){
  l_coefs$zero[i] <- lm.beta(sim_res_4$results[[i]]$gimme$reg_outstrength[[1]])$standardized.coefficients[2]
  l_coefs$two[i] <- lm.beta(sim_res_4$results[[i]]$gimme$reg_outstrength[[2]])$standardized.coefficients[2]
  l_coefs$four[i] <- lm.beta(sim_res_4$results[[i]]$gimme$reg_outstrength[[3]])$standardized.coefficients[2]
}

l_coefs |> 
  dplyr::bind_rows(.id = "true_coef") |> 
    pivot_longer(cols = everything(), 
               names_to = "reg_coef") |> 
  mutate(reg_coef = as.factor(reg_coef)) |> 
  ggplot(aes(x = reg_coef, y = value)) + 
  geom_point()
```
